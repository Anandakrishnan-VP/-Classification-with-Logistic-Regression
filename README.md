Breast Cancer Classification with Logistic Regression
ðŸ“Œ Overview
This project uses the Breast Cancer Wisconsin Dataset to build a binary classification model that predicts whether a tumor is Malignant (M) or Benign (B) using Logistic Regression.

We go step-by-step through:

Data loading & cleaning

Train-test splitting & standardization

Model training

Model evaluation (confusion matrix, precision, recall, ROC-AUC)

Threshold tuning & sigmoid function explanation

ðŸ“‚ Dataset
Source: Kaggle â€“ Breast Cancer Wisconsin (Diagnostic) dataset

File Used: breast_cancer.csv

Target Column:

diagnosis â†’ M = Malignant (1), B = Benign (0)

Shape after cleaning:

Training set: (455, 30)

Test set: (114, 30)

âš™ï¸ Steps
1. Load and Clean Data
python
Copy
Edit
import pandas as pd
df = pd.read_csv("breast_cancer.csv")
df.drop(["Unnamed: 32", "id"], axis=1, inplace=True)
y = df["diagnosis"].map({"M": 1, "B": 0})
X = df.drop("diagnosis", axis=1)
2. Train-Test Split
python
Copy
Edit
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
3. Standardize Features
python
Copy
Edit
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
4. Train Logistic Regression Model
python
Copy
Edit
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
5. Model Evaluation
Confusion Matrix â€“ visualizes TP, FP, TN, FN

Precision â€“ proportion of predicted positives that are correct

Recall â€“ proportion of actual positives correctly detected

ROC-AUC â€“ measures the modelâ€™s ability to distinguish classes

ðŸ“ˆ Results
Metric	Value
Precision	0.97
Recall	0.93
ROC-AUC	1.00

Threshold tuning results
Threshold	Precision	Recall
0.3	0.98	0.98
0.5 (default)	0.97	0.93
0.7	1.00	0.90

Insights:

Lower threshold (0.3) increases recall, catching more malignant cases.

Higher threshold (0.7) increases precision, reducing false positives but missing some true positives.

In medical contexts, higher recall is often preferred to minimize missed cancer detections.

ðŸ§® Sigmoid Function in Logistic Regression
Logistic Regression outputs a probability between 0 and 1 using the sigmoid (logistic) function:

ðœŽ
(
ð‘§
)
=
1
1
+
ð‘’
âˆ’
ð‘§
Ïƒ(z)= 
1+e 
âˆ’z
 
1
â€‹
 
Where:

ð‘§
=
ð‘¤
1
ð‘¥
1
+
ð‘¤
2
ð‘¥
2
+
â‹¯
+
ð‘
z=w 
1
â€‹
 x 
1
â€‹
 +w 
2
â€‹
 x 
2
â€‹
 +â‹¯+b (linear combination of inputs)

Output = probability that the instance belongs to class 1 (Malignant in this case)

Key properties:

Always between 0 and 1

Output > 0.5 â†’ predict class 1

Output < 0.5 â†’ predict class 0

Smooth curve, useful for threshold tuning

ðŸ“Š Visualizations
Confusion Matrix

ROC Curve

ðŸ“Œ How to Run
bash
Copy
Edit
pip install pandas numpy scikit-learn matplotlib seaborn
python breast_cancer_logreg.py
ðŸ§  Key Takeaways
Logistic regression is fast, interpretable, and effective for binary classification.

Always scale features before training.

Threshold tuning is crucial when false negatives have serious consequences (e.g., cancer detection).

The sigmoid function is the mathematical heart of logistic regression, mapping any real number into a probability.

